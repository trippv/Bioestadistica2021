---
title: "Clase14_PCA"
author: "Miguel Tripp"
date: "2021-07-22"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


# 1: Generalidades

 
**¿Que es un PCA?**

El análisis de componentes principales (_Principal Component Analysis_) o **PCA** es una herramienta para el análisis exploratorio de los datos que permite visualizar la variación presente de un set de datos con muchas variables.


De manera general, el PCA es un tipo de transformación lineal de un set de datos con un cierto número de variables. Dicha transformación ajusta el est de datos a un nuevo sistema de coordenadas de manera que la mayor propoción de la varianza se explica en la primera coordenada y cada coordenada subsiguiente es ortogonal a la anterior y explica una menor variabilidad. 


**¿Cuando se usa un PCA?**

Una de las principales aplicaciones de PCA es la **reducción de dimensionalidad** (es decir, variables): cuando contamos con un gran número de variables cuantitativs posiblemente correlacionadas (indicativo de exstencia de información redundante), un PCA permite "reducirlas" a un número menor de variables transformadas (componentes principales) que expliquen gran parte de la variabilidad de los datos. 

Cada dimensión o componente princpal generado por PCA será una combinación lineal de las variables originales. El PCA puede considerarse como una rotación de los ejes del sistema de coordenadas de las variables originales a nuevos ejes ortogonales, de manera que estos ejes coincidan con la dirección de máxima varianza de los datos.

Otro aspecto importante de un PCA es que no se especifica una variable de respuesta, es decir, no se especifica el origen de los datos. Pero tras describir la varianza de los datos usando las combinaciones lineales del PCA es posible distinuir grupos separados entre los individuos. Es por este motivo que se le conoce como método no supervisado. 

Es importante recordar que el PCA reduce la dimensionalidad pero **no reduce el número de variables en los datos**. Esto significa que puedes explicar el 99% de la variabilidad de un set de datos con 1,000 variables usando solamente los tres componentes principales, pero aún necesitas esos 1,000 variables para construir los componentes principales. 

## 2: Metabolitos de abulon

```{r message= FALSE}
library(tidyverse)
```

```{r}
metabolitos <- read_csv("data/Hful_metabolitos_ver2.csv")
```

# 2.1: Normalización de los datos

Un aspecto fundamenta al aplicar un PCA es que las observaciones tienen que moverse al centro del eje de coordenadas, esto es, centrarlas para que tengan media de 0, para asi eliminar posibles sesgos en las mediciones. 

Los datos también se escalan a una varianza unitaria para eliminar el efecto de las distintas unidades en las que puedan estar medidas los datos. 

Este proceso de normalización y escalar se aplican directamente dentro de la función para hacer el PCA, pero para visualizar este efecto, se hara el siguiente ejercicio:

Primero es necesario generar un nuevo objeto transformalo a formato largo con `gather()`.


```{r}
metabolitos_long <- metabolitos[,-1] %>%
  gather(key = "metabolito", value, -Group)


ggplot(metabolitos_long, aes(y = metabolito, x = value))+
  geom_boxplot()

ggplot(metabolitos_long, aes(x = value))+
  geom_density()
```

Posteriomente generamos un objeto con las variables normalizadas
```{r}
metabolitos_long_scale <- scale(log10(metabolitos[,-c(1:2)]), center = TRUE, scale = TRUE)

metabolitos_long_scale <- data.frame(metabolitos_long_scale) %>%                 
                  gather(key = "metabolito", value) 

ggplot(metabolitos_long_scale, aes(y = metabolito, x = value))+
  geom_boxplot()

ggplot(metabolitos_long_scale, aes(x = value))+
  geom_density()
```

## 2.2: PCA con `prcomp()`

```{r}
metabolitos_pca <- prcomp(log10(metabolitos[,-c(1:2)]), center = TRUE, scale. = TRUE)


summary(metabolitos_pca)
```



En este caso obtuvimos 15 componentes principales. Cada uno de ellos explica cierto porcentaje de la variación total del set de datos. Es decir, el PC1 explica 52% de la varianza total, lo cual implica que mas de la mitad de la información e los datos puede encapsularse en ese componente principal. PC2 explica 15% de la varianza, por lo que ambos componentes explicar un 68% de la varianza total.


```{r}
screeplot(metabolitos_pca, type = "l", main = "scree plot con los componentes principales")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalor = 1"),
       col=c("red"), lty=5, cex=0.6)
```

O podemos calcular manualmente la proporción de la varianza explicada por cada componente


La varianza explicada por cada componente principal (correspondiente a los eigenvalores) la obtenemos elevando al cuadrado la desviación estándar.

```{r}
prop_varianza <- metabolitos_pca$sdev^2 / sum(metabolitos_pca$sdev^2)

prop_varianza <- data.frame(prop_varianza, pc = 1:12)


```

```{r}
ggplot(prop_varianza, aes(x = pc, y = prop_varianza))+
  geom_bar(stat = "identity")+
  theme_bw()+
  labs(x = "Componente principal", y = "Prop. de varianza explicada")
```

y la proporción acumulada de la varianza

```{r}
prop_varianza$accum <- cumsum(prop_varianza$prop_varianza)
```

```{r}
ggplot(prop_varianza, aes(x = pc, y = accum))+
  geom_point()+
  geom_line()+
  geom_label(aes(label = round(accum,2))) +
  theme_bw()+
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```


 
 Como es de esperar, la varianza explicada es mayor en la primera componente que en las subsiguientes.


No existe un método objetivo para escoger el número de componentes principales que son suficientes para un análisis, por lo que depende del juicio del analista y del problema en cuestión. Si explican suficiente variabilidad y el objetivo es la visualización de los datos, no se suelen escoger más de tres componentes principales, para así facilitar la representación gráfica y la interpretación. 


```{r}
str(metabolitos_pca)
```

El objeto prcomp contiene la información necesario: 

  * Centro (`$center`), la escala (`scale`), desviación estandar (`$dev`) de cada componente principal.

  * La relación (correlación o anticorrelación) etre las variables iniciales y el componente principal (`$rotation`)

  * El valor de cada muestra en función del componente principal

Los elementos **center** y **scale** se corresponden con las medias y las desviaciones estándar originales de las variables previo escalado e implementación del PCA. 

La matriz **rotation** proporciona los _loadings_ de los componentes principales (cada columna contiene el vector de loadings de cada componente principal). La función los denomina matriz de rotación ya que si multiplicáramos la matriz de datos por `$rotation`, obtendríamos las coordenadas de los datos en el nuevo sistema rotado de coordenadas. Estas coordenadas se corresponden con los **scores** de los componentes principales.

De manera que:
```{r}
metabolitos_pca$rotation
```

el primer componente es el resultado de la siguiente combinación lineal de las variables originales:

$PC1 = -0.117406 ATP - 0.244613 Acetate + 0.219658 Alanine... etc$


## 2.3: Visualización del PCA

Una de las formas de visualizar los resultados de un PCA es mediante el llamado `biplot`. Este permite visualizar la posición de cada muestra en relación al PC1 y PC2 y como contribuye cada variable a cada componente principal. 

```{r}
biplot(metabolitos_pca, scale = 0)
```

La comparación de la distancia entre puntuaciones y vectores no es relevante en la interpretación de los biplots, teniendo en cuenta que hay varias versiones de biplots que se pueden obtener según como se escalen sus elementos (ya que esto afecta a la compactación o dispersión de la representación). Las interpretaciones se centran en las direcciones y agrupamientos tanto de unos como de otros.

Para los **vectores** (variables), nos fijamos en su longitud y en el ángulo con respecto a los ejes de las componentes principales y entre ellos mismos:

 * _Ángulo_: cuanto más paralelo es un vector al eje de una componente, más ha contribuido a la creación de la misma. Con ello obtienes información sobre qué variable(s) ha sido más determinante para crear cada componente, y si entre las variables (y cuales) hay correlaciones. Ángulos pequeños entre vectores representa alta correlación entre las variables implicadas (observaciones con valores altos en una de esas variables tendrá valores altos en la variable o variables correlacionadas); ángulos rectos representan falta de correlación, y ángulos opuestos representan correlación negativa (una observación con valores altos en una de las variables irá acompañado de valores bajos en la otra).

  * _Longitud_: cuanto mayor la longitud de un vector relacionado con x variable (en un rango normalizado de 0 a 1), mayor variabilidad de dicha variable está contenida en la representación de las dos componentes del biplot, es decir, mejor está representada su información en el gráfico.

Para los **scores** (observaciones), nos fijamos en los posibles agrupamientos. Puntuaciones próximas representan observaciones de similares características. Puntuaciones con valores de las variables próximas a la media se sitúan más cerca del centro del biplot (0, 0). El resto representan variabilidades normales o extremas (outliers). Por otro lado, la relación de las observaciones con las variables se puede estudiar proyectando las observaciones sobre la dirección de los vectores[[1](https://rpubs.com/Cristina_Gil/PCA)].





Cada punto es una observación de nuestra tabla. Cada observación esta gráficada con su valor de PC1 y PC2. Los ejes se muestran como flechas originarias del centro. En est caso se observa que, por ejemplo, metabolitos como la `taurina`, `isoleucina`o `carnitina`contribuyen al PC1 con menores valores en las variables que meuven a las muestras hacia la izquierda del gráfica. Por otro lado, metabolitos como la `alanina` se incrementan con el PC2. 


EL paso siguiente es extraer los PCs 1 y 2, los cuales son los que explican la mayor variabilidad de los datos y graficarlo usarlo ggplot. Para esto, necesitamos extraer los PC 1 y 2 de nuestro objecto pca y unirlos a nuestra tabla original de metabolitos



```{r}

metabolitos_PCs <- cbind(metabolitos, metabolitos_pca$x[, 1:2])

head(metabolitos_PCs)
```

Ahora graficamos con ggplot

```{r}
ggplot(metabolitos_PCs, aes(x = PC1, y = PC2, col = Group, fill = Group))+
stat_ellipse(geom = "polygon", col = "black", alpha = 0.3)+
  geom_point(shape = 21, col = "black")
```


Alternativamente, podemos usar el paquete ggbiplot para graficar el biplot
```{r eval = FALSE}
library(devtools)
install_github("vqv/ggbiplot")
```

```{r message= FALSE}
library(ggbiplot)

ggbiplot(metabolitos_pca, labels = metabolitos$ind.names, ellipse = TRUE, groups = metabolitos$Group, scale = 0)+
  theme_bw()
```


Con esta función tambien es posible graficar el segundo y tercer componente principal (PC2 y PC3), los cuales explican el 15% y 8.7% de la variabildiad de los datos, respectivamente.

```{r}
ggbiplot(metabolitos_pca, choices = c(2,3), labels = metabolitos$ind.names, ellipse = TRUE, groups = metabolitos$Group, scale = 0)+
  theme_bw()
```


```{r}
ggbiplot(metabolitos_pca, choices = c(1,2), labels = metabolitos$ind.names, ellipse = TRUE, groups = metabolitos$Group, scale = 0, var.axes = FALSE)+
  theme_bw()
```

